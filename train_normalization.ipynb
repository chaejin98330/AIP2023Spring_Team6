{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YWXJdNjmTYF-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "import tqdm\n",
        "from tqdm.notebook import tqdm as notebooktqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import timm\n",
        "import os\n",
        "# from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# work place\n",
        "work_dir = './'\n",
        "os.chdir(work_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tgP-pcsnhB9M"
      },
      "outputs": [],
      "source": [
        "class YoutubeDataset(Dataset):\n",
        "    def __init__(self, data, doc2vec):\n",
        "        self.ids = list(data['video_id'])\n",
        "        self.titles = doc2vec\n",
        "        self.data = data\n",
        "        self.image_encoder = timm.create_model('efficientnet_b1_pruned', features_only =True, pretrained=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.ids[idx]\n",
        "        \n",
        "        image = Image.open( work_dir+'medium_15287/{}.jpg'.format(video_id))\n",
        "        image = torch.FloatTensor(np.array(image)).permute(2, 0, 1).unsqueeze(dim=0)\n",
        "        self.image_encoder.eval()\n",
        "        feature_map = self.image_encoder(image)[-1].squeeze() # (320,6,10)\n",
        "        \n",
        "        title = self.titles[video_id]\n",
        "        title = torch.FloatTensor(np.array(title, dtype=np.float16))\n",
        "        \n",
        "        meta = torch.FloatTensor(self.data[['period_day', 'subscriber_count']].to_numpy()[idx])\n",
        "        \n",
        "        y = np.log10(self.data['views'].to_numpy() + 1)\n",
        "        y = np.expand_dims(y, axis=1)\n",
        "        y = torch.FloatTensor(y[idx])\n",
        "        \n",
        "        return video_id, feature_map, title, meta, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0izhvsIDjxhZ",
        "outputId": "764d0746-418f-4afe-c9f2-fdad39f7a88c"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "360Pm1IVTtmM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "335.8148713475796 0.0 1784323.5617822357 0\n",
            "Train Dataset Size :  1000\n",
            "Validation Dataset Size :  100\n",
            "Test Dataset Size :  100\n"
          ]
        }
      ],
      "source": [
        "# add nomarlizing\n",
        "data = pd.read_csv('./train.csv')\n",
        "max_period = data['period_day'].mean()\n",
        "min_period = data['period_day'].min()\n",
        "max_sub = data['subscriber_count'].mean()\n",
        "min_sub = data['subscriber_count'].min()\n",
        "print(max_period, min_period, max_sub, min_sub)\n",
        "\n",
        "data['period_day'] = data['period_day']/max_period\n",
        "data['subscriber_count'] = data['subscriber_count']/max_sub\n",
        "\n",
        "train_data, valid_data = train_test_split(data, test_size = 0.1, random_state = 55)\n",
        "test_data = pd.read_csv('./test.csv')\n",
        "train_data = train_data[:1000]\n",
        "valid_data = valid_data[:100]\n",
        "test_data = test_data[:100]\n",
        "print('Train Dataset Size : ',len(train_data))\n",
        "print('Validation Dataset Size : ',len(valid_data))\n",
        "print('Test Dataset Size : ',len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15287\n"
          ]
        }
      ],
      "source": [
        "# open doc2vec data and conver to dict\n",
        "with open('./title_doc2vec_10', 'rb') as f:\n",
        "    doc2vec = pickle.load(f)\n",
        "\n",
        "data_dict=dict()\n",
        "for row in doc2vec:\n",
        "    vid=row[0]\n",
        "    vec=row[1:]\n",
        "    data_dict[vid]=vec\n",
        "\n",
        "doc2vec = data_dict\n",
        "print(len(doc2vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RaVBtO6gmibN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#setting hyper parameters\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "lr = 1e-4\n",
        "save_every = 5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_d3vE8CgwOQK"
      },
      "outputs": [],
      "source": [
        "train_dataset = YoutubeDataset(train_data, doc2vec)\n",
        "valid_dataset = YoutubeDataset(valid_data, doc2vec)\n",
        "test_dataset = YoutubeDataset(test_data, doc2vec)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = 1)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.feature_map_channel = 320\n",
        "        self.feature_map_h = 6\n",
        "        self.feature_map_w = 10\n",
        "        self.bottle_nect_out_channel = 10\n",
        "\n",
        "        self.title_channel = 10\n",
        "\n",
        "        self.img_title_concat_hidden = 50\n",
        "        self.img_title_concat_out = 10\n",
        "\n",
        "        self.final_concat_hidden = 20\n",
        "        self.final_concat_out = 1\n",
        "        \n",
        "        \n",
        "        self.squeeze_conv = nn.Conv2d(in_channels=self.feature_map_channel, out_channels=self.bottle_nect_out_channel, kernel_size=3, padding=1) \n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=(self.feature_map_h, self.feature_map_w), stride=1)\n",
        "        self.img_title_concat_fc1 = nn.Linear(self.bottle_nect_out_channel+self.title_channel, self.img_title_concat_hidden)\n",
        "        self.img_title_concat_fc2 = nn.Linear(self.img_title_concat_hidden, self.img_title_concat_out)\n",
        "\n",
        "        self.final_concat_fc1 = nn.Linear(self.img_title_concat_out+2, self.final_concat_hidden)\n",
        "        self.final_concat_fc2 = nn.Linear(self.final_concat_hidden, self.final_concat_out)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(19212, 500)\n",
        "        self.fc2 = nn.Linear(500, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, feature_map, title, meta):\n",
        "        feature_map = feature_map.to(self.device)\n",
        "        title = title.to(self.device)\n",
        "        meta = meta.to(self.device)\n",
        "\n",
        "        squeezed_feature_map = self.squeeze_conv(feature_map)\n",
        "        squeezed_feature_map = self.relu(squeezed_feature_map)\n",
        "        dense_feature_map = torch.squeeze(self.max_pool(squeezed_feature_map), dim = (2,3))\n",
        "\n",
        "        img_title_feature = torch.cat([dense_feature_map, title], dim=1)\n",
        "        img_title_feature = self.img_title_concat_fc1(img_title_feature)\n",
        "        img_title_feature = self.relu(img_title_feature)\n",
        "        img_title_feature = self.img_title_concat_fc2(img_title_feature)\n",
        "        img_title_feature = self.relu(img_title_feature)\n",
        "        img_title_feature = self.dropout(img_title_feature)\n",
        "\n",
        "        whole_feature = torch.cat([img_title_feature, meta], dim=1)\n",
        "        whole_feature = self.final_concat_fc1(whole_feature)\n",
        "        whole_feature = self.relu(whole_feature)\n",
        "        whole_feature = self.dropout(whole_feature)\n",
        "        x = self.final_concat_fc2(whole_feature)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def train_(self, epochs, lr, train_loader, valid_loader, save_every):\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.valid_loss = []\n",
        "\n",
        "        best_mse = 1e100\n",
        "        best_epoch = 1\n",
        "\n",
        "        train_start = time.time()\n",
        "\n",
        "        print(\"Model will be trained on {}\\n\".format(self.device))\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            self.train()\n",
        "            print(\"[Epoch {:3d} / {}]\".format(epoch, epochs))\n",
        "\n",
        "            epoch_start = time.time()\n",
        "            epoch_loss = 0.0\n",
        "            \n",
        "            #training\n",
        "            for batch_idx, batch_data in enumerate(tqdm.tqdm(train_loader, desc=\"Training\")):\n",
        "                batch_video_id, batch_image, batch_title, batch_meta, batch_target = batch_data\n",
        "                batch_target = batch_target.to(self.device)\n",
        "                \n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.forward(batch_image, batch_title, batch_meta)\n",
        "                loss = self.criterion(output, batch_target)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                print('Epoch {}, mini-batch {}, loss {}'.format(epoch, batch_idx, loss.item()))\n",
        "\n",
        "                # just for debuging\n",
        "                break\n",
        "\n",
        "            epoch_end = time.time()\n",
        "            m, s = divmod(epoch_end - epoch_start, 60)\n",
        "\n",
        "            epoch_loss /= len(train_loader)\n",
        "            self.train_loss.append(epoch_loss)\n",
        "            \n",
        "            #validation\n",
        "            with torch.no_grad():\n",
        "                self.eval()\n",
        "                true_y, pred_y = self.predict(valid_loader)                \n",
        "                true_y = torch.FloatTensor(true_y).unsqueeze(dim=1)\n",
        "                pred_y = torch.FloatTensor(pred_y)\n",
        "                valid_loss = self.criterion(pred_y, true_y)\n",
        "                self.valid_loss.append(valid_loss.item())\n",
        "\n",
        "            print(\"Train MSE = {:.4f} | Valid MSE = {:.4f}\".format(epoch_loss, valid_loss))\n",
        "            print(f\"Train Time: {m:.0f}m {s:.0f}s\\n\")\n",
        "\n",
        "            valid_mse = valid_loss.item()\n",
        "            if best_mse > valid_mse:\n",
        "                print(\"=> Best Model Updated : Epoch = {}, Valid MSE = {:.4f}\\n\".format(epoch, valid_mse))\n",
        "                best_mse = valid_mse\n",
        "                best_epoch = epoch\n",
        "                torch.save(self.state_dict(), \"./best_model/best_model.pt\")\n",
        "            else:\n",
        "                print()\n",
        "\n",
        "            # save model for every ? epoch\n",
        "            if (epoch % save_every) == 0:\n",
        "                torch.save(self.state_dict(),\"./model/epoch{}_train{:.4f}_valid{:.4f}.pt\".format(epoch, epoch_loss, valid_mse))\n",
        "\n",
        "        m, s = divmod(time.time() - train_start, 60)\n",
        "        print(\"\\nTraining Finished...!!\")\n",
        "        print(\"\\nBest Valid MSE : %.2f at epoch %d\" % (best_mse, best_epoch))\n",
        "        print(f\"Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {self.device}!\")\n",
        "\n",
        "        torch.save(self.state_dict(),\"./model/epoch{}_train{:.4f}_valid{:.4f}.pt\".format(epoch, epoch_loss, valid_mse))\n",
        "    \n",
        "    def restore(self):\n",
        "        with open(\"./best_model/best_model.pt\", \"rb\") as f:\n",
        "            state_dict = torch.load(f)\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "    def predict(self, dataloader):\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "            true_y = []\n",
        "            pred_y = []\n",
        "            for batch_video_id, batch_image, batch_title, batch_meta, batch_target in dataloader:\n",
        "                pred = self.forward(batch_image, batch_title, batch_meta)\n",
        "                true_y.append(batch_target.numpy())\n",
        "                pred_y.append(pred.cpu().numpy())\n",
        "            true_y = np.concatenate(true_y, axis=0).squeeze()\n",
        "            pred_y = np.concatenate(pred_y, axis=0)\n",
        "        return true_y, pred_y #numpy array\n",
        "\n",
        "    def plot(self):\n",
        "        plt.plot(np.array(self.train_loss), \"b\")\n",
        "        plt.plot(np.array(self.valid_loss), \"g\")\n",
        "        plt.savefig(\"./graph.png\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Vn1sTA9K-DBX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model will be trained on cpu\n",
            "\n",
            "[Epoch   1 / 100]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/16 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m Model()\n\u001b[0;32m      2\u001b[0m model\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49mtrain_(epochs, lr, train_loader, valid_loader, save_every)\n",
            "Cell \u001b[1;32mIn[25], line 79\u001b[0m, in \u001b[0;36mModel.train_\u001b[1;34m(self, epochs, lr, train_loader, valid_loader, save_every)\u001b[0m\n\u001b[0;32m     76\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[39m#training\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm\u001b[39m.\u001b[39mtqdm(train_loader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m     80\u001b[0m     batch_video_id, batch_image, batch_title, batch_meta, batch_target \u001b[39m=\u001b[39m batch_data\n\u001b[0;32m     81\u001b[0m     batch_target \u001b[39m=\u001b[39m batch_target\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[19], line 17\u001b[0m, in \u001b[0;36mYoutubeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(np\u001b[39m.\u001b[39marray(image))\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_encoder\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> 17\u001b[0m feature_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_encoder(image)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msqueeze() \u001b[39m# (320,6,10)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m title \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtitles[video_id]\n\u001b[0;32m     20\u001b[0m title \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(np\u001b[39m.\u001b[39marray(title, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat16))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\timm\\models\\efficientnet.py:260\u001b[0m, in \u001b[0;36mEfficientNetFeatures.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    258\u001b[0m     x \u001b[39m=\u001b[39m checkpoint(b, x)\n\u001b[0;32m    259\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     x \u001b[39m=\u001b[39m b(x)\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stage_out_idx:\n\u001b[0;32m    262\u001b[0m     features\u001b[39m.\u001b[39mappend(x)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\timm\\models\\_efficientnet_blocks.py:184\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    182\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_dw(x)\n\u001b[0;32m    183\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n\u001b[1;32m--> 184\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mse(x)\n\u001b[0;32m    185\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_pwl(x)\n\u001b[0;32m    186\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(x)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\timm\\models\\_efficientnet_blocks.py:52\u001b[0m, in \u001b[0;36mSqueezeExcite.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     51\u001b[0m     x_se \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmean((\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m), keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 52\u001b[0m     x_se \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_reduce(x_se)\n\u001b[0;32m     53\u001b[0m     x_se \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact1(x_se)\n\u001b[0;32m     54\u001b[0m     x_se \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_expand(x_se)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = Model()\n",
        "model.to(model.device)\n",
        "model.train_(epochs, lr, train_loader, valid_loader, save_every)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
