{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4297,"status":"ok","timestamp":1683952242796,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"cRdpc7aMxXWc","outputId":"a19cbfd3-d0c5-40b5-de44-7b964692ba0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu for inference\n"]}],"source":["# import package\n","\n","# model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","\n","# dataset and transformation\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision import models\n","\n","# display images\n","from torchvision import utils\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# utils\n","import numpy as np\n","from glob import glob\n","import os\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f'Using {device} for inference')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":612,"status":"ok","timestamp":1683952243387,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"LGLTLgTq8_Ki","outputId":"c44d9153-9ac6-4599-a945-57684b689338"},"outputs":[{"name":"stdout","output_type":"stream","text":["csv len: 15287\n","train len:  13758 \n","test len:  1529\n"]},{"data":{"text/plain":["\"\\nfor test in tqdm(test_list):\\n  if os.path.exists('./medium_15287/' + test + '.jpg'):  \\n    img_path = glob('./medium_15287/' + test + '.jpg')[0]\\n    shutil.copyfile(img_path, './test/' + test + '.jpg')\\n  \\n  else:\\n    print(test)\\n\\nfor train in tqdm(train_list):\\n  if os.path.exists('./medium_15287/' + train + '.jpg'):  \\n    img_path = glob('./medium_15287/' + train + '.jpg')[0]\\n    shutil.copyfile(img_path, './train/' + train + '.jpg')\\n  else:\\n    print(train)\\n\""]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# train test split\n","\n","import csv\n","import shutil\n","import pandas as pd\n","\n","train_list = []\n","test_list = []\n","test_views=[]\n","train_views=[]\n","\n","train_csv=pd.read_csv('train.csv',index_col=0)\n","train_list=train_csv['video_id'].values.tolist()\n","train_views=train_csv['views'].values.tolist()\n","\n","test_csv=pd.read_csv('test.csv',index_col=0)\n","test_list=test_csv['video_id'].values.tolist()\n","test_views=test_csv['views'].values.tolist()\n","#full_data_path = './data/'\n","\n","\n","\n","print(f'csv len: {len(test_list) + len(train_list)}')\n","print(\"train len: \", len(train_list), \"\\ntest len: \", len(test_list))\n","# print(f'full data len: {len(os.listdir(\"./data\"))}')\n","'''\n","for test in tqdm(test_list):\n","  if os.path.exists('./medium_15287/' + test + '.jpg'):  \n","    img_path = glob('./medium_15287/' + test + '.jpg')[0]\n","    shutil.copyfile(img_path, './test/' + test + '.jpg')\n","  \n","  else:\n","    print(test)\n","\n","for train in tqdm(train_list):\n","  if os.path.exists('./medium_15287/' + train + '.jpg'):  \n","    img_path = glob('./medium_15287/' + train + '.jpg')[0]\n","    shutil.copyfile(img_path, './train/' + train + '.jpg')\n","  else:\n","    print(train)\n","'''\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["test len: 1529\n","train len: 13758\n"]}],"source":["print(f'test len: {len(os.listdir(\"./test\"))}')\n","print(f'train len: {len(os.listdir(\"./train\"))}')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(13758,)\n","(1529,)\n"]}],"source":["train_views=(train_views-np.mean(train_views))/np.std(train_views)\n","print(train_views.shape)\n","\n","test_views=(test_views-np.mean(test_views))/np.std(test_views)\n","print(test_views.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1683952248621,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"KBm_91Y_dWL5"},"outputs":[],"source":["from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class SampleDataset(Dataset):\n","    def __init__(self, phase='test'):\n","        # self.path = './sample_data'\n","        self.phase = phase\n","        \n","        self.img_list = glob(self.phase + '/*')\n","\n","        self.transform = transform=transforms.Compose([\n","                               transforms.ToTensor(),\n","                               transforms.Resize((180,320), antialias=True),\n","                               transforms.Pad(padding=(0, 140), padding_mode='reflect'),\n","                               transforms.Resize((224, 224), antialias=True)\n","                         ])\n","    \n","    def __len__(self):\n","        return len(self.img_list)\n","    \n","    def __getitem__(self, idx):\n","        img_path = self.img_list[idx]\n","\n","        img = Image.open(img_path)\n","        img = self.transform(img)\n","\n","        vid = img_path.split('/')[-1][:-4]\n","        views=0\n","        if self.phase==\"test\":\n","            views=test_views[idx]\n","        else:\n","            views=train_views[idx]\n","        \n","        return img, vid, views"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":833,"status":"ok","timestamp":1683952249451,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"EqZCd5hpywDR"},"outputs":[],"source":["# Prepare sample input data.\n","\n","batch_size = 64\n","\n","test_dataset = SampleDataset(phase='test')\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=40)\n","\n","train_dataset = SampleDataset(phase='train')\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=40)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.9694, 0.9821, 0.9888,  ..., 0.9144, 0.8754, 0.8657],\n","         [0.9952, 0.9892, 0.9779,  ..., 0.8600, 0.7740, 0.6905],\n","         [0.8666, 0.7739, 0.6570,  ..., 0.5535, 0.4480, 0.3702],\n","         ...,\n","         [0.7195, 0.7526, 0.8177,  ..., 0.3175, 0.3325, 0.3279],\n","         [0.6894, 0.7431, 0.9138,  ..., 0.2987, 0.3502, 0.3404],\n","         [0.7049, 0.7226, 0.7907,  ..., 0.3351, 0.3166, 0.3465]],\n","\n","        [[0.3721, 0.3869, 0.3909,  ..., 0.6986, 0.6914, 0.7082],\n","         [0.3757, 0.3834, 0.3950,  ..., 0.6919, 0.6135, 0.5401],\n","         [0.3835, 0.3123, 0.2354,  ..., 0.4575, 0.3525, 0.2713],\n","         ...,\n","         [0.7074, 0.7247, 0.7555,  ..., 0.3762, 0.3960, 0.3937],\n","         [0.6747, 0.7126, 0.8514,  ..., 0.3551, 0.4122, 0.4042],\n","         [0.6880, 0.6891, 0.7238,  ..., 0.3880, 0.3784, 0.4109]],\n","\n","        [[0.3939, 0.4086, 0.4174,  ..., 0.3540, 0.3901, 0.4149],\n","         [0.4267, 0.4292, 0.4363,  ..., 0.5980, 0.5611, 0.5182],\n","         [0.4215, 0.3443, 0.2592,  ..., 0.4640, 0.3337, 0.2854],\n","         ...,\n","         [0.6226, 0.6471, 0.6930,  ..., 0.4785, 0.5040, 0.5063],\n","         [0.6044, 0.6476, 0.7985,  ..., 0.4409, 0.5038, 0.4996],\n","         [0.6337, 0.6408, 0.6864,  ..., 0.4551, 0.4503, 0.4847]]])\n","91lkJlrIVpA tensor(-0.0303, dtype=torch.float64)\n"]}],"source":["sample=iter(train_dataloader)\n","img, vid, views=next(sample)\n","print(img[0])\n","print(vid[0], views[0])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5236,"status":"ok","timestamp":1683952248621,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"XWT6QQ1cyY2G","outputId":"0e1def69-eff9-4182-9951-8da4b84cb9cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained weights for efficientnet-b0\n","Linear(in_features=1280, out_features=1000, bias=True)\n","Sequential(\n","  (0): Linear(in_features=1280, out_features=100, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=100, out_features=1, bias=True)\n",")\n"]}],"source":["#train encoder\n","from efficientnet_pytorch import EfficientNet\n","efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n","in_features=efficientnet._fc.in_features\n","print(efficientnet._fc)\n","efficientnet._fc=nn.Sequential(\n","    nn.Linear(in_features,100,bias=True),\n","    nn.ReLU(),\n","    nn.Linear(100,1)    \n",")\n","print(efficientnet._fc)\n","\n","      \n","layer=0\n","for child in efficientnet.children():\n","    if layer<7:\n","        for param in child.parameters():\n","            param.requires_grad=False\n","    layer+=1\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["<torch.utils.hooks.RemovableHandle at 0x7ff1385a16f0>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["activation={}\n","def get_activation(name):\n","    def hook(model, input, output):\n","        activation[name] = output.detach()\n","    return hook\n","\n","efficientnet._fc[0].register_forward_hook(get_activation('fc'))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 3, 224, 224])\n","tensor([[ 0.1377, -0.0235,  0.1240,  ..., -0.2328, -0.2142,  0.1410],\n","        [-0.0212, -0.1182, -0.0881,  ...,  0.2749,  0.0510, -0.2523],\n","        [-0.0353,  0.3424,  0.0714,  ..., -0.0347, -0.1519, -0.4871],\n","        ...,\n","        [-0.2779,  0.1164, -0.2016,  ...,  0.0545,  0.0324, -0.1957],\n","        [-0.3449,  0.0016,  0.3101,  ...,  0.7043, -0.3968,  0.1066],\n","        [-0.2233,  0.0491, -0.0651,  ..., -0.1581, -0.2054, -0.0356]]) torch.Size([64, 100])\n"]}],"source":["sample=iter(train_dataloader)\n","batch=next(sample)\n","print(batch[0].shape)\n","efficientnet(batch[0])\n","tmp=activation['fc']\n","\n","print(tmp, tmp.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["_fc.0.weight Parameter containing:\n","tensor([[ 0.0087,  0.0118, -0.0184,  ...,  0.0154,  0.0194, -0.0065],\n","        [ 0.0233,  0.0211, -0.0115,  ..., -0.0247,  0.0083,  0.0003],\n","        [ 0.0269,  0.0268,  0.0121,  ...,  0.0160,  0.0141, -0.0192],\n","        ...,\n","        [ 0.0268, -0.0225, -0.0260,  ...,  0.0097,  0.0092,  0.0256],\n","        [ 0.0275,  0.0163,  0.0048,  ...,  0.0272, -0.0039,  0.0226],\n","        [ 0.0001,  0.0092,  0.0204,  ...,  0.0037,  0.0110,  0.0168]],\n","       requires_grad=True)\n","_fc.0.bias Parameter containing:\n","tensor([ 0.0020,  0.0262, -0.0011, -0.0017,  0.0271, -0.0242, -0.0144, -0.0256,\n","        -0.0046, -0.0270, -0.0238, -0.0166,  0.0057, -0.0206, -0.0267, -0.0047,\n","        -0.0094, -0.0184, -0.0121, -0.0029,  0.0187,  0.0010, -0.0072, -0.0210,\n","        -0.0069, -0.0007, -0.0218, -0.0268,  0.0238, -0.0237, -0.0246, -0.0221,\n","        -0.0127,  0.0061, -0.0031,  0.0145, -0.0140,  0.0213, -0.0109, -0.0243,\n","         0.0119, -0.0211,  0.0226,  0.0153,  0.0262,  0.0212,  0.0084, -0.0225,\n","        -0.0258,  0.0090, -0.0061,  0.0201,  0.0188,  0.0097,  0.0133,  0.0027,\n","         0.0005,  0.0241, -0.0198,  0.0130, -0.0202,  0.0085,  0.0071, -0.0015,\n","         0.0073, -0.0197, -0.0055, -0.0099, -0.0202,  0.0061, -0.0168, -0.0158,\n","        -0.0121, -0.0181,  0.0212,  0.0060, -0.0188,  0.0150,  0.0266, -0.0031,\n","         0.0050,  0.0194,  0.0278,  0.0157,  0.0238,  0.0193, -0.0082, -0.0009,\n","        -0.0278,  0.0044,  0.0251, -0.0070, -0.0144,  0.0016, -0.0238,  0.0116,\n","        -0.0123, -0.0235,  0.0216,  0.0091], requires_grad=True)\n","_fc.2.weight Parameter containing:\n","tensor([[ 0.0554,  0.0709,  0.0791, -0.0234,  0.0236, -0.0252, -0.0521, -0.0192,\n","          0.0515, -0.0019, -0.0746,  0.0718,  0.0507, -0.0722,  0.0720, -0.0928,\n","         -0.0024,  0.0894, -0.0444,  0.0770,  0.0757, -0.0537, -0.0447,  0.0550,\n","         -0.0588, -0.0267, -0.0772,  0.0124, -0.0063,  0.0546,  0.0957,  0.0211,\n","          0.0855,  0.0058, -0.0387,  0.0737, -0.0663, -0.0746,  0.0788,  0.0850,\n","         -0.0353, -0.0652, -0.0765,  0.0324,  0.0727, -0.0015, -0.0833,  0.0667,\n","          0.0320,  0.0235, -0.0869, -0.0415,  0.0178, -0.0123,  0.0813,  0.0399,\n","          0.0222, -0.0795, -0.0772, -0.0501, -0.0245,  0.0409,  0.0552, -0.0113,\n","         -0.0692,  0.0827,  0.0614, -0.0869,  0.0686, -0.0062,  0.0594,  0.0993,\n","         -0.0017, -0.0390,  0.0146,  0.0350,  0.0474,  0.0043, -0.0065,  0.0972,\n","         -0.0281, -0.0682, -0.0251,  0.0319,  0.0974,  0.0901,  0.0048, -0.0773,\n","          0.0621, -0.0550,  0.0672, -0.0347,  0.0534, -0.0475, -0.0004,  0.0142,\n","         -0.0728,  0.0335,  0.0163, -0.0204]], requires_grad=True)\n","_fc.2.bias Parameter containing:\n","tensor([-0.0925], requires_grad=True)\n"]}],"source":["loss_f=nn.MSELoss()\n","params_to_update=[]\n","for name, param in efficientnet.named_parameters():\n","    if param.requires_grad==True:\n","        params_to_update.append(param)\n","        print(name, param)\n","        \n","optim=torch.optim.SGD(params_to_update, lr=0.0001, momentum=0.8)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1:  19%|█▉        | 41/215 [01:03<04:24,  1.52s/batch, loss=0.402] "]}],"source":["epochs=5\n","\n","for epoch in range(epochs):    \n","    efficientnet.train()\n","    with tqdm(train_dataloader, unit=\"batch\") as batch:\n","        for img, vid, views in batch:\n","            batch.set_description(f\"Epoch {epoch+1}\")\n","            preds=efficientnet(img).flatten()\n","            \n","            batch_loss=loss_f(preds.to(torch.float32),views.to(torch.float32))\n","            \n","            optim.zero_grad()\n","            batch_loss.backward()\n","            optim.step()\n","            batch.set_postfix(loss=batch_loss.item())\n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUsP3Ih2RPBr"},"outputs":[],"source":["save_data = {}\n","\n","with torch.no_grad():\n","  for img, vid, views in tqdm(train_dataloader):\n","    \n","    out = efficientnet(img)\n","    # print(features.shape)\n","    act=activation['fc']\n","\n","    print(act, act.shape)\n","    for b in range(batch_size):\n","      if b < len(vid):\n","        save_data[vid[b]] = act[b]\n","\n","print(f'train len: {len(train_list)}')\n","print(f'data len: {len(save_data)}')\n","\n","with open('train.pickle', 'wb') as f:\n","  pickle.dump(save_data, f, pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Cgjc5WvUOTg_"},"source":["데이터 로더 안쓰는 버전"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrhRukTVI4Xo"},"outputs":[],"source":["# # without dataloader ver\n","\n","# from PIL import Image\n","# import pickle\n","\n","# save_data = {}\n","\n","# transform = transform=transforms.Compose([\n","#                                transforms.ToTensor(),\n","#                                transforms.Pad(padding=(0, 140), padding_mode='reflect'),\n","#                                transforms.Resize((224, 224))\n","#                       ])\n","\n","# for vid in tqdm(test_list):\n","#   img_path = './test/' + vid + '.jpg'\n","\n","#   img = Image.open(img_path)\n","#   img = transform(img).unsqueeze(0)\n","\n","#   # print(img.shape)\n","\n","#   features = efficientnet.extract_features(img)\n","#   # print(features.shape)\n","\n","#   save_data[vid] = features\n","\n","# with open('test.pickle', 'wb') as f:\n","#   pickle.dump(save_data, f, pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96CIwfune6Ku"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPTdAg29CyTRuwN9GIaiPHd","gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
