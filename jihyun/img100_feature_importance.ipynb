{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5-1Q1MWmQrC",
        "outputId": "6337e81c-dec0-4871-b053-aa0f7eac306a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "O1sn5dbemee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5821940f-abf3-4860-9426-b933d1131ed6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.14.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/인지프'\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "IinldbEdm2jM",
        "outputId": "920fd55b-6955-4266-af6b-db51497c4c27"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/인지프\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/인지프'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YWXJdNjmTYF-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "import tqdm\n",
        "from tqdm.notebook import tqdm as notebooktqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import timm\n",
        "from timm.layers import BatchNormAct2d\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dkHljvsPmMVd"
      },
      "outputs": [],
      "source": [
        "# work place\n",
        "work_dir = './'\n",
        "os.chdir(work_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tgP-pcsnhB9M"
      },
      "outputs": [],
      "source": [
        "class YoutubeDataset(Dataset):\n",
        "    def __init__(self, data, doc2vec):\n",
        "        self.ids = list(data['video_id'])\n",
        "        self.titles = doc2vec # pretrained doc2vec features\n",
        "        self.data = data # video_id, metadata, views(y) from csv file\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.image_encoder = timm.create_model('efficientnet_b1_pruned', features_only =True, pretrained=True)\n",
        "        model = timm.create_model('efficientnet_b1_pruned', pretrained=True)\n",
        "        data_cfg = timm.data.resolve_data_config(model.pretrained_cfg)\n",
        "        self.transform = timm.data.create_transform(**data_cfg)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # index order of video_id, meta, y are same\n",
        "        video_id = self.ids[idx]\n",
        "        \n",
        "        image = Image.open( work_dir+'medium_15287/{}.jpg'.format(video_id))\n",
        "        image = self.transform(image)\n",
        "        # image = torch.FloatTensor(np.array(image)).permute(2, 0, 1).unsqueeze(dim=0)\n",
        "        self.image_encoder.eval()\n",
        "        feature_map = self.image_encoder(torch.unsqueeze(image,0))[-1].squeeze() # (320,6,10)\n",
        "        \n",
        "        title = self.titles[video_id] # get video title\n",
        "        title = torch.FloatTensor(np.array(title, dtype=np.float16))\n",
        "        \n",
        "        meta = torch.FloatTensor(self.data[['period_day', 'subscriber_count']].to_numpy()[idx]) # get metadata\n",
        "        \n",
        "        y = np.log10(self.data['views'].to_numpy() + 1) # add 1 for zero views\n",
        "        y = np.expand_dims(y, axis=1) # add batch dimension\n",
        "        y = torch.FloatTensor(y[idx]) # get log10(views+1) by idx value\n",
        "        \n",
        "        return video_id, feature_map, title, meta, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "360Pm1IVTtmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c876612-5afd-4c1d-cf29-42978a2a36c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "335.8148713475796 497.7613157973895 1784323.5617822357 3833786.6144638904\n",
            "Train Dataset Size :  12382\n",
            "Validation Dataset Size :  1376\n",
            "Test Dataset Size :  1529\n"
          ]
        }
      ],
      "source": [
        "# add nomarlizing\n",
        "data = pd.read_csv('./train.csv')\n",
        "mean_period = data['period_day'].mean()\n",
        "std_period = data['period_day'].std()\n",
        "mean_sub = data['subscriber_count'].mean()\n",
        "std_sub = data['subscriber_count'].std()\n",
        "print(mean_period, std_period, mean_sub, std_sub)\n",
        "\n",
        "data['period_day'] = (data['period_day'] - mean_period)/std_period\n",
        "data['subscriber_count'] = (data['subscriber_count']-mean_sub)/std_sub\n",
        "\n",
        "train_data, valid_data = train_test_split(data, test_size = 0.1, random_state = 55)\n",
        "test_data = pd.read_csv('./test.csv')\n",
        "# train_data = train_data[:1000]\n",
        "# valid_data = valid_data[:100]\n",
        "# test_data = test_data[:100]\n",
        "print('Train Dataset Size : ',len(train_data))\n",
        "print('Validation Dataset Size : ',len(valid_data))\n",
        "print('Test Dataset Size : ',len(test_data))\n",
        "\n",
        "# data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkO7xevpmMVf",
        "outputId": "25f9c888-a85b-4e08-8b42-619a51423402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15287\n"
          ]
        }
      ],
      "source": [
        "# open doc2vec data and conver to dict\n",
        "with open('./title_doc2vec_10', 'rb') as f:\n",
        "    doc2vec = pickle.load(f)\n",
        "\n",
        "data_dict=dict()\n",
        "for row in doc2vec:\n",
        "    vid=row[0]\n",
        "    vec=row[1:]\n",
        "    data_dict[vid]=vec\n",
        "\n",
        "doc2vec = data_dict\n",
        "print(len(doc2vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pyrX1NZgmMVf"
      },
      "outputs": [],
      "source": [
        "#setting hyper parameters\n",
        "batch_size = 64\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_d3vE8CgwOQK"
      },
      "outputs": [],
      "source": [
        "train_dataset = YoutubeDataset(train_data, doc2vec)\n",
        "valid_dataset = YoutubeDataset(valid_data, doc2vec)\n",
        "test_dataset = YoutubeDataset(test_data, doc2vec)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = 1)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -o ./medium_15287.zip -d ./medium_15287\n",
        "\n",
        "data_list = os.listdir('./medium_15287')\n",
        "print(len(data_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjQqBHHUpCRp",
        "outputId": "c11f6195-e4aa-4aa5-829c-dffc56c41b51"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EcodingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EcodingModel, self).__init__()\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.feature_map_channel = 320\n",
        "        self.feature_map_h = 6\n",
        "        self.feature_map_w = 10\n",
        "\n",
        "        self.efficient_net_channel1 = 1280\n",
        "        self.efficient_net_channel2 = 1000\n",
        "\n",
        "        # image squeezing\n",
        "        self.img_squeeze_channel1 = self.efficient_net_channel2\n",
        "        self.img_squeeze_channel2 = 2000\n",
        "        self.img_squeeze_channel3 = 1000\n",
        "        self.img_squeeze_channel4 = 500\n",
        "        self.img_squeeze_channel_out = 100\n",
        "\n",
        "        # title squeezing\n",
        "        self.title_feature_channel = 10\n",
        "        self.title_squeeze_channel1 = 200\n",
        "        self.title_squeeze_channel2 = 100\n",
        "        self.title_squeeze_channel3 = 50\n",
        "        self.title_squeeze_channel_out = 10\n",
        "        \n",
        "        # efficient net\n",
        "        self.effi1 = nn.Conv2d(self.feature_map_channel, self.efficient_net_channel1, kernel_size=(1,1), stride=(1,1), bias=False)\n",
        "        self.effi2 = nn.BatchNorm2d(self.efficient_net_channel1, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.effi3 = nn.SiLU(inplace=True)\n",
        "        self.effi4 = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.effi5 = nn.Linear(self.efficient_net_channel1, self.efficient_net_channel2)\n",
        "        \n",
        "        # sqeeze img features\n",
        "        self.img_squeeze_fc1 = nn.Linear(self.img_squeeze_channel1, self.img_squeeze_channel2)\n",
        "        self.img_squeeze_fc2 = nn.Linear(self.img_squeeze_channel2, self.img_squeeze_channel3)\n",
        "        self.img_squeeze_fc3 = nn.Linear(self.img_squeeze_channel3, self.img_squeeze_channel4)\n",
        "        self.img_squeeze_fc_out = nn.Linear(self.img_squeeze_channel4, self.img_squeeze_channel_out)\n",
        " \n",
        "        # sqeeze img and title features\n",
        "        self.title_squeeze_fc1 = nn.Linear(self.img_squeeze_channel_out+self.title_feature_channel, self.title_squeeze_channel1)\n",
        "        self.title_squeeze_fc2 = nn.Linear(self.title_squeeze_channel1, self.title_squeeze_channel2)\n",
        "        self.title_squeeze_fc3 = nn.Linear(self.title_squeeze_channel2, self.title_squeeze_channel3)\n",
        "        self.title_squeeze_fc_out = nn.Linear(self.title_squeeze_channel3, self.title_squeeze_channel_out)\n",
        " \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        self.to(self.device)\n",
        "    \n",
        "    def forward(self, feature_map, title, meta):\n",
        "        feature_map = feature_map.to(self.device)\n",
        "        title = title.to(self.device)\n",
        "        meta = meta.to(self.device)\n",
        "\n",
        "        x = self.effi1(feature_map)\n",
        "        x = self.effi2(x)\n",
        "        x = self.effi3(x)\n",
        "        x = torch.squeeze(self.effi4(x), dim=(2,3))\n",
        "        x = self.effi5(x)\n",
        "\n",
        "        x = self.img_squeeze_fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.img_squeeze_fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.img_squeeze_fc3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.img_squeeze_fc_out(x)  # output size: 100\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x) \n",
        "\n",
        "        # img_title_feature = torch.cat([x, title], dim=1)\n",
        "        # img_title_feature = self.title_squeeze_fc1(img_title_feature)\n",
        "        # img_title_feature = self.dropout(img_title_feature)\n",
        "        # img_title_feature = self.relu(img_title_feature)\n",
        "        # img_title_feature = self.title_squeeze_fc2(img_title_feature)\n",
        "        # img_title_feature = self.dropout(img_title_feature)\n",
        "        # img_title_feature = self.relu(img_title_feature)\n",
        "        # img_title_feature = self.title_squeeze_fc3(img_title_feature)\n",
        "        # img_title_feature = self.dropout(img_title_feature)\n",
        "        # img_title_feature = self.relu(img_title_feature)\n",
        "        # img_title_feature = self.title_squeeze_fc_out(img_title_feature)  # output size: 10\n",
        "        # img_title_feature = self.dropout(img_title_feature)\n",
        "        # x = self.relu(img_title_feature)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def encode(self, dataloader):\n",
        "        device = self.device\n",
        "\n",
        "        self.to(device)\n",
        "        with torch.no_grad():\n",
        "            self.eval()\n",
        "\n",
        "            img_features = []\n",
        "            titles = []\n",
        "            metas = []\n",
        "            targets = []\n",
        "\n",
        "            for _, batch_image, batch_title, batch_meta, batch_target in dataloader:\n",
        "                batch_image = batch_image.to(device)\n",
        "                batch_title = batch_title.to(device)\n",
        "                batch_meta = batch_meta.to(device)\n",
        "\n",
        "                img_feature = self.forward(batch_image, batch_title, batch_meta)\n",
        "\n",
        "                img_features.append(img_feature)\n",
        "                titles.append(batch_title)\n",
        "                metas.append(batch_meta)\n",
        "                targets.append(batch_target)\n",
        "\n",
        "            img_features = torch.cat(img_features, dim=0).cpu().numpy()\n",
        "            titles = torch.cat(titles, dim=0).cpu().numpy()\n",
        "            metas = torch.cat(metas, dim=0).cpu().numpy()\n",
        "            targets = torch.cat(targets, dim=0).cpu().numpy()\n",
        "\n",
        "        return img_features, titles, metas, targets\n"
      ],
      "metadata": {
        "id": "IKQfNaL76RkC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uHhKlECGmMVg"
      },
      "outputs": [],
      "source": [
        "encoder = EcodingModel()\n",
        "encoder.to(encoder.device)\n",
        "train_img, train_title, train_meta, train_target = encoder.encode(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "btVZecWRmMVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafe00bb-a9ef-41f1-9b46-c240b6cbbbf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12382, 112)\n"
          ]
        }
      ],
      "source": [
        "data = np.concatenate((train_img, train_title, train_meta), axis=1)\n",
        "train_df = pd.DataFrame(data, columns=['img_{}'.format(i) for i in range(100)] + ['title_{}'.format(i) for i in range(10)] + ['period_day', 'subscriber_count'])\n",
        "\n",
        "train_df.head()\n",
        "print(train_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rHYIeq87mMVh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "901315b4-e2a8-492a-c5e8-f38d2d45d5dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "              objective='multi:softprob', predictor=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "\n",
        "xgbc = XGBClassifier(random_state=111)\n",
        "xgbc.fit(train_df, np.array(train_target, dtype='int'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_impt = xgbc.feature_importances_\n",
        "\n",
        "img_impt = np.sum(feature_impt[:100])\n",
        "title_impt = np.sum(feature_impt[100:110])\n",
        "period_impt = feature_impt[-2]\n",
        "subscriber_impt = feature_impt[-1]\n",
        "\n",
        "print(img_impt + title_impt + period_impt + subscriber_impt)\n",
        "\n",
        "# plot_importance(xgbc, max_num_features=22)\n",
        "\n",
        "plt.barh(['image', 'title' , 'period', 'subscribers'], [img_impt, title_impt, period_impt, subscriber_impt])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "S_awVcT3IoTs",
        "outputId": "b74e72f4-938e-445b-bfc0-c0c6c5c270c6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0000001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}