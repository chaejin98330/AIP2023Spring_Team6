{"cells":[{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4297,"status":"ok","timestamp":1683952242796,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"cRdpc7aMxXWc","outputId":"a19cbfd3-d0c5-40b5-de44-7b964692ba0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda for inference\n"]}],"source":["# import package\n","\n","# model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","import timm\n","\n","# dataset and transformation\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision import models\n","\n","# display images\n","from torchvision import utils\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# utils\n","import numpy as np\n","from glob import glob\n","import os\n","from tqdm import tqdm\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","\n","device=torch.device('cpu')\n","if torch.backends.mps.is_available():\n","    device=torch.device('mps')\n","elif torch.cuda.is_available():\n","    device=torch.device('cuda')\n","print(f'Using {device} for inference')"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":612,"status":"ok","timestamp":1683952243387,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"LGLTLgTq8_Ki","outputId":"c44d9153-9ac6-4599-a945-57684b689338"},"outputs":[{"name":"stdout","output_type":"stream","text":["csv len: 15287\n","train len:  13758 \n","test len:  1529\n"]}],"source":["# train test split\n","\n","import csv\n","import shutil\n","import pandas as pd\n","\n","train_list = []\n","test_list = []\n","\n","train_csv=pd.read_csv('train.csv',index_col=0)\n","train_list=train_csv['video_id'].values.tolist()\n","\n","test_csv=pd.read_csv('test.csv',index_col=0)\n","test_list=test_csv['video_id'].values.tolist()\n","#full_data_path = './data/'\n","\n","\n","\n","print(f'csv len: {len(test_list) + len(train_list)}')\n","print(\"train len: \", len(train_list), \"\\ntest len: \", len(test_list))\n","# print(f'full data len: {len(os.listdir(\"./data\"))}')\n","\n","# for test in tqdm(test_list):\n","#   if os.path.exists('./medium_15287/' + test + '.jpg'):  \n","#     img_path = glob('./medium_15287/' + test + '.jpg')[0]\n","#     shutil.copyfile(img_path, './test/' + test + '.jpg')\n","  \n","#   else:\n","#     print(test)\n","\n","# for train in tqdm(train_list):\n","#   if os.path.exists('./medium_15287/' + train + '.jpg'):  \n","#     img_path = glob('./medium_15287/' + train + '.jpg')[0]\n","#     shutil.copyfile(img_path, './train/' + train + '.jpg')\n","#   else:\n","#     print(train)\n","\n","\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["test len: 1529\n","train len: 13758\n"]}],"source":["print(f'test len: {len(os.listdir(\"./test\"))}')\n","print(f'train len: {len(os.listdir(\"./train\"))}')"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1683952248621,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"KBm_91Y_dWL5"},"outputs":[],"source":["from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class SampleDataset(Dataset):\n","    def __init__(self, phase='test'):\n","        # self.path = './sample_data'\n","        self.phase = phase\n","        \n","        self.img_list = glob(self.phase + '/*')\n","\n","        self.transform =transforms.Compose([\n","            transforms.Resize(size=272, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, max_size=None, antialias=True),\n","            transforms.CenterCrop(size=(240, 240)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=torch.tensor([0.5000, 0.5000, 0.5000]), std=torch.tensor([0.5000, 0.5000, 0.5000]))\n","        ])\n","    \n","    def __len__(self):\n","        return len(self.img_list)\n","    \n","    def __getitem__(self, idx):\n","        img_path = self.img_list[idx]\n","\n","        img = Image.open(img_path)\n","        img = self.transform(img)\n","\n","        vid = img_path.split('/')[-1][:-4]\n","        \n","        return img, vid"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":833,"status":"ok","timestamp":1683952249451,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"EqZCd5hpywDR"},"outputs":[],"source":["# Prepare sample input data.\n","\n","batch_size = 64\n","\n","test_dataset = SampleDataset(phase='test')\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","train_dataset = SampleDataset(phase='train')\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["import copy\n","import logging\n","import timm\n","import pathlib\n","\n","_logger = logging.getLogger(__name__)\n","\n","def my_create_timm_model(**init_args):\n","    # HACK: fix the bug for feature_only=True and checkpoint_path != \"\"\n","    # https://github.com/rwightman/pytorch-image-models/issues/488\n","    if init_args.get(\"checkpoint_path\", \"\") != \"\" and init_args.get(\"features_only\", True):\n","        init_args = copy.deepcopy(init_args)\n","        full_model_name = init_args[\"model_name\"]\n","        modules = timm.models.list_modules()\n","        # find the mod which has the longest common name in model_name\n","        mod_len = 0\n","        for m in modules:\n","            if m in full_model_name:\n","                cur_mod_len = len(m)\n","                if cur_mod_len > mod_len:\n","                    mod = m\n","                    mod_len = cur_mod_len\n","        if mod_len >= 1:\n","            if hasattr(timm.models.__dict__[mod], \"default_cfgs\"):\n","                ckpt_path = init_args.pop(\"checkpoint_path\")\n","                ckpt_url = pathlib.Path(ckpt_path).resolve().as_uri()\n","                _logger.warning(f\"hacking model pretrained url to {ckpt_url}\")\n","                timm.models.__dict__[mod].default_cfgs[full_model_name][\"url\"] = ckpt_url\n","                init_args[\"pretrained\"] = True\n","        else:\n","            raise ValueError(f\"model_name {full_model_name} has no module in timm\")\n","\n","    backbone = timm.create_model(**init_args)\n","    return backbone"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5236,"status":"ok","timestamp":1683952248621,"user":{"displayName":"이지현","userId":"00379118698049138207"},"user_tz":-540},"id":"XWT6QQ1cyY2G","outputId":"0e1def69-eff9-4182-9951-8da4b84cb9cf"},"outputs":[{"name":"stderr","output_type":"stream","text":["hacking model pretrained url to file:///C:/Users/iblue/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/OneDrive%20-%20%EC%84%B1%EA%B7%A0%EA%B4%80%EB%8C%80%ED%95%99%EA%B5%90/%ED%95%99%EA%B5%90%EA%B3%BC%EC%A0%9C/%EC%9D%B8%EC%A7%80%ED%94%84/feature_autoencoder.ipynb/effnetb1_pruned-bea43a3a.pth\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 240, 240])\n","torch.Size([1, 16, 120, 120])\n","torch.Size([1, 12, 60, 60])\n","torch.Size([1, 35, 30, 30])\n","torch.Size([1, 67, 15, 15])\n","torch.Size([1, 320, 8, 8])\n","4\n"]}],"source":["#train encoder\n","# import os\n","# os.environ['TORCH_HOME'] = './'\n","\n","efficientnet = my_create_timm_model(model_name='efficientnet_b1_pruned', pretrained=True, features_only=True, checkpoint_path='./effnetb1_pruned-bea43a3a.pth')\n","\n","test_input=torch.randn(1,3,240,240)\n","print(test_input.shape)\n","o=efficientnet(test_input)\n","for f in o:\n","    print(f.shape)\n","layers=0\n","for child in efficientnet.children():\n","    for param in child.parameters():\n","        param.requires_grad=False\n","    layers+=1\n","print(layers)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["reduced_vector=1000\n","\n","class DeepAutoencoder(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()        \n","        self.efficientnet = efficientnet\n","\n","        self.encoder=torch.nn.Sequential(\n","            torch.nn.Flatten(),\n","            torch.nn.Linear(320*8*8,reduced_vector*4),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(reduced_vector*4, reduced_vector)\n","        )\n","          \n","        self.decoder = torch.nn.Sequential(\n","            torch.nn.Linear(reduced_vector, reduced_vector*4),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(reduced_vector*4, 320*8*8),\n","        )\n","  \n","    def forward(self, x):\n","        feature_map=self.efficientnet(x)[-1]\n","        encoded = self.encoder(feature_map)\n","        decoded = self.decoder(encoded)\n","        return decoded, feature_map\n","  \n","# Instantiating the model and hyperparameters\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["model = DeepAutoencoder()\n","model.to(device)\n","criterion = torch.nn.MSELoss()\n","params_to_update=[]\n","\n","for name, param in model.encoder.named_parameters():\n","    if param.requires_grad==True:\n","        params_to_update.append(param)\n","\n","for name, param in model.decoder.named_parameters():\n","    if param.requires_grad==True:\n","        params_to_update.append(param)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.Adam(params_to_update, lr=0.0001)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["num_epochs=500"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 215/215 [06:48<00:00,  1.90s/batch, loss=7.67]\n","Test Epoch 1: 100%|██████████| 24/24 [00:37<00:00,  1.54s/batch, loss=7.83]\n","Epoch 2: 100%|██████████| 215/215 [01:17<00:00,  2.78batch/s, loss=6.66]\n","Test Epoch 2: 100%|██████████| 24/24 [00:07<00:00,  3.13batch/s, loss=7.03]\n","Epoch 3: 100%|██████████| 215/215 [01:19<00:00,  2.72batch/s, loss=6.08]\n","Test Epoch 3: 100%|██████████| 24/24 [00:08<00:00,  2.99batch/s, loss=6.61]\n","Epoch 4: 100%|██████████| 215/215 [01:19<00:00,  2.71batch/s, loss=5.68]\n","Test Epoch 4: 100%|██████████| 24/24 [00:08<00:00,  2.93batch/s, loss=6.34]\n","Epoch 5: 100%|██████████| 215/215 [01:19<00:00,  2.70batch/s, loss=5.38]\n","Test Epoch 5: 100%|██████████| 24/24 [00:08<00:00,  2.83batch/s, loss=6.16]\n","Epoch 6: 100%|██████████| 215/215 [01:19<00:00,  2.70batch/s, loss=5.17]\n","Test Epoch 6: 100%|██████████| 24/24 [00:07<00:00,  3.02batch/s, loss=6.03]\n","Epoch 7: 100%|██████████| 215/215 [01:20<00:00,  2.68batch/s, loss=4.99]\n","Test Epoch 7: 100%|██████████| 24/24 [00:07<00:00,  3.02batch/s, loss=5.93]\n","Epoch 8: 100%|██████████| 215/215 [01:20<00:00,  2.67batch/s, loss=4.89]\n","Test Epoch 8: 100%|██████████| 24/24 [00:09<00:00,  2.67batch/s, loss=5.89]\n","Epoch 9: 100%|██████████| 215/215 [01:21<00:00,  2.64batch/s, loss=4.77]\n","Test Epoch 9: 100%|██████████| 24/24 [00:08<00:00,  2.97batch/s, loss=5.83]\n","Epoch 10: 100%|██████████| 215/215 [01:19<00:00,  2.71batch/s, loss=4.7] \n","Test Epoch 10: 100%|██████████| 24/24 [00:07<00:00,  3.04batch/s, loss=5.81]\n","Epoch 11: 100%|██████████| 215/215 [01:19<00:00,  2.70batch/s, loss=4.62]\n","Test Epoch 11: 100%|██████████| 24/24 [00:07<00:00,  3.01batch/s, loss=5.81]\n","Epoch 12:  60%|██████    | 129/215 [00:48<00:32,  2.66batch/s, loss=4.59]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[49], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(train_dataloader, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m batch:\n\u001b[1;32m----> 7\u001b[0m     \u001b[39mfor\u001b[39;00m img, vid \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m      8\u001b[0m         batch\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m         img\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mto(device)\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[41], line 26\u001b[0m, in \u001b[0;36mSampleDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_list[idx]\n\u001b[0;32m     25\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\n\u001b[1;32m---> 26\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m     28\u001b[0m vid \u001b[39m=\u001b[39m img_path\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m img, vid\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[0;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2156\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   2154\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(size)\n\u001b[1;32m-> 2156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   2157\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2158\u001b[0m     box \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n","File \u001b[1;32mc:\\Users\\iblue\\anaconda3\\lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["best_loss = 1e9\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(num_epochs):    \n","    model.train()\n","    train_loss = 0\n","\n","    with tqdm(train_dataloader, unit=\"batch\") as batch:\n","        for img, vid in batch:\n","            batch.set_description(f\"Epoch {epoch+1}\")\n","            img=img.to(device)\n","            preds, feat_map=model(img)\n","\n","            preds=preds.flatten()\n","            feat_map=feat_map.flatten()\n","            \n","            batch_loss=criterion(preds.to(torch.float32),feat_map.to(torch.float32))\n","            train_loss += batch_loss.item()\n","            \n","            optimizer.zero_grad()\n","            batch_loss.backward()\n","            optimizer.step()\n","            batch.set_postfix(loss=batch_loss.item())\n","\n","    train_losses.append(train_loss / len(train_dataloader))\n","\n","    model.eval()\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        with tqdm(test_dataloader, unit=\"batch\") as batch:\n","            for img, vid in batch:\n","                batch.set_description(f\"Test Epoch {epoch+1}\")\n","                img=img.to(device)\n","                preds, feat_map=model(img)\n","\n","                preds=preds.flatten()\n","                feat_map=feat_map.flatten()\n","                \n","                batch_loss=criterion(preds.to(torch.float32),feat_map.to(torch.float32))\n","                val_loss += batch_loss.item()\n","                \n","                batch.set_postfix(loss=batch_loss.item())\n","\n","    loss = val_loss / len(test_dataloader)\n","    val_losses.append(loss)\n","\n","    # graw loss graph\n","    x = range(epoch + 1)\n","    plt.plot(x, train_losses, x, val_losses)\n","    plt.legend(['train_loss', 'val_loss'])\n","    plt.savefig('loss_graph.png')\n","\n","    # save log\n","    with open('log.txt', '+a') as f:\n","        f.write(f'Epoch: {epoch} | loss: {loss}\\n')\n","\n","        if loss < best_loss:\n","            torch.save(model, f'autoencoder_best.pickle')\n","            f.write(f'Save Model in epoch {epoch}\\n')\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model, 'autoencoder.pickle')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##END OF CODE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUsP3Ih2RPBr"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/215 [00:00<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"Mismatched Tensor types in NNPack convolutionOutput","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m   \u001b[39mfor\u001b[39;00m img, vid \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m----> 5\u001b[0m     out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mefficientnet(img)\n\u001b[1;32m      6\u001b[0m     out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencoder(out)\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/timm/models/efficientnet.py:604\u001b[0m, in \u001b[0;36mEfficientNetFeatures.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 604\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_stem(x)\n\u001b[1;32m    605\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    606\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_hooks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/timm/models/layers/conv2d_same.py:30\u001b[0m, in \u001b[0;36mConv2dSame.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m conv2d_same(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/timm/models/layers/conv2d_same.py:17\u001b[0m, in \u001b[0;36mconv2d_same\u001b[0;34m(x, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconv2d_same\u001b[39m(\n\u001b[1;32m     14\u001b[0m         x, weight: torch\u001b[39m.\u001b[39mTensor, bias: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, stride: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m     15\u001b[0m         padding: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), dilation: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), groups: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     16\u001b[0m     x \u001b[39m=\u001b[39m pad_same(x, weight\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:], stride, dilation)\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(x, weight, bias, stride, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), dilation, groups)\n","\u001b[0;31mRuntimeError\u001b[0m: Mismatched Tensor types in NNPack convolutionOutput"]}],"source":["# save_data = {}\n","\n","# with torch.no_grad():\n","#   for img, vid in tqdm(train_dataloader):\n","#     out = model.efficientnet(img)\n","#     out = model.encoder(out)\n","#     for b in range(batch_size):\n","#       if b < len(vid):\n","#         save_data[vid[b]] = out[b]\n","\n","# print(f'train len: {len(train_list)}')\n","# print(f'data len: {len(save_data)}')\n","\n","# with open('train.pickle', 'wb') as f:\n","#   pickle.dump(save_data, f, pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# with torch.no_grad():\n","#   for img, vid in tqdm(test_dataloader):\n","#     out = model.efficientnet(img)\n","#     out = model.encoder(out)\n","#     print(out, out.shape)\n","#     for b in range(batch_size):\n","#       if b < len(vid):\n","#         save_data[vid[b]] = out[b]\n","\n","# print(f'test len: {len(train_list)}')\n","# print(f'data len: {len(save_data)}')\n","\n","# with open('test.pickle', 'wb') as f:\n","#   pickle.dump(save_data, f, pickle.HIGHEST_PROTOCOL)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Cgjc5WvUOTg_"},"source":["데이터 로더 안쓰는 버전"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrhRukTVI4Xo"},"outputs":[],"source":["# # without dataloader ver\n","\n","# from PIL import Image\n","# import pickle\n","\n","# save_data = {}\n","\n","# transform = transform=transforms.Compose([\n","#                                transforms.ToTensor(),\n","#                                transforms.Pad(padding=(0, 140), padding_mode='reflect'),\n","#                                transforms.Resize((224, 224))\n","#                       ])\n","\n","# for vid in tqdm(test_list):\n","#   img_path = './test/' + vid + '.jpg'\n","\n","#   img = Image.open(img_path)\n","#   img = transform(img).unsqueeze(0)\n","\n","#   # print(img.shape)\n","\n","#   features = efficientnet.extract_features(img)\n","#   # print(features.shape)\n","\n","#   save_data[vid] = features\n","\n","# with open('test.pickle', 'wb') as f:\n","#   pickle.dump(save_data, f, pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96CIwfune6Ku"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPTdAg29CyTRuwN9GIaiPHd","gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
